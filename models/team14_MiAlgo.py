import collections.abc
import math
import warnings
from itertools import repeat

import torch
import torch.nn as nn
from einops import rearrange
from torch import nn as nn
from torch.nn import functional as F


def get_hat():
    ##############################################################
    ################              HAT                 ############
    ##############################################################
    # https://github.com/XPixelGroup/HAT
    def _no_grad_trunc_normal_(tensor, mean, std, a, b):
        def norm_cdf(x):
            # Computes standard normal cumulative distribution function
            return (1. + math.erf(x / math.sqrt(2.))) / 2.

        if (mean < a - 2 * std) or (mean > b + 2 * std):
            warnings.warn(
                'mean is more than 2 std from [a, b] in nn.init.trunc_normal_. '
                'The distribution of values may be incorrect.',
                stacklevel=2)

        with torch.no_grad():
            # Values are generated by using a truncated uniform distribution and
            # then using the inverse CDF for the normal distribution.
            # Get upper and lower cdf values
            low = norm_cdf((a - mean) / std)
            up = norm_cdf((b - mean) / std)

            # Uniformly fill tensor with values from [low, up], then translate to
            # [2l-1, 2u-1].
            tensor.uniform_(2 * low - 1, 2 * up - 1)

            # Use inverse cdf transform for normal distribution to get truncated
            # standard normal
            tensor.erfinv_()

            # Transform to proper mean, std
            tensor.mul_(std * math.sqrt(2.))
            tensor.add_(mean)

            # Clamp to ensure it's in the proper range
            tensor.clamp_(min=a, max=b)
            return tensor

    def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
        r"""Fills the input Tensor with values drawn from a truncated
        normal distribution.

        From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py

        The values are effectively drawn from the
        normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
        with values outside :math:`[a, b]` redrawn until they are within
        the bounds. The method used for generating the random values works
        best when :math:`a \leq \text{mean} \leq b`.

        Args:
            tensor: an n-dimensional `torch.Tensor`
            mean: the mean of the normal distribution
            std: the standard deviation of the normal distribution
            a: the minimum cutoff value
            b: the maximum cutoff value

        Examples:
            >>> w = torch.empty(3, 5)
            >>> nn.init.trunc_normal_(w)
        """
        return _no_grad_trunc_normal_(tensor, mean, std, a, b)

    # From PyTorch
    def _ntuple(n):

        def parse(x):
            if isinstance(x, collections.abc.Iterable):
                return x
            return tuple(repeat(x, n))

        return parse

    to_1tuple = _ntuple(1)
    to_2tuple = _ntuple(2)
    to_3tuple = _ntuple(3)
    to_4tuple = _ntuple(4)
    to_ntuple = _ntuple

    def drop_path(x, drop_prob: float = 0., training: bool = False):
        """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

        From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py
        """
        if drop_prob == 0. or not training:
            return x
        keep_prob = 1 - drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()  # binarize
        output = x.div(keep_prob) * random_tensor
        return output

    class DropPath(nn.Module):
        """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).

        From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py
        """

        def __init__(self, drop_prob=None):
            super(DropPath, self).__init__()
            self.drop_prob = drop_prob

        def forward(self, x):
            return drop_path(x, self.drop_prob, self.training)

    class ChannelAttention(nn.Module):
        """Channel attention used in RCAN.
        Args:
            num_feat (int): Channel number of intermediate features.
            squeeze_factor (int): Channel squeeze factor. Default: 16.
        """

        def __init__(self, num_feat, squeeze_factor=16):
            super(ChannelAttention, self).__init__()
            self.attention = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(num_feat, num_feat // squeeze_factor, 1, padding=0),
                nn.ReLU(inplace=True),
                nn.Conv2d(num_feat // squeeze_factor, num_feat, 1, padding=0),
                nn.Sigmoid())

        def forward(self, x):
            y = self.attention(x)
            return x * y

    class CAB(nn.Module):

        def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):
            super(CAB, self).__init__()

            self.cab = nn.Sequential(
                nn.Conv2d(num_feat, num_feat // compress_ratio, 3, 1, 1),
                nn.GELU(),
                nn.Conv2d(num_feat // compress_ratio, num_feat, 3, 1, 1),
                ChannelAttention(num_feat, squeeze_factor)
            )

        def forward(self, x):
            return self.cab(x)

    class Mlp(nn.Module):

        def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
            super().__init__()
            out_features = out_features or in_features
            hidden_features = hidden_features or in_features
            self.fc1 = nn.Linear(in_features, hidden_features)
            self.act = act_layer()
            self.fc2 = nn.Linear(hidden_features, out_features)
            self.drop = nn.Dropout(drop)

        def forward(self, x):
            x = self.fc1(x)
            x = self.act(x)
            x = self.drop(x)
            x = self.fc2(x)
            x = self.drop(x)
            return x

    def window_partition(x, window_size):
        """
        Args:
            x: (b, h, w, c)
            window_size (int): window size

        Returns:
            windows: (num_windows*b, window_size, window_size, c)
        """
        b, h, w, c = x.shape
        x = x.view(b, h // window_size, window_size, w // window_size, window_size, c)
        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, c)
        return windows

    def window_reverse(windows, window_size, h, w):
        """
        Args:
            windows: (num_windows*b, window_size, window_size, c)
            window_size (int): Window size
            h (int): Height of image
            w (int): Width of image

        Returns:
            x: (b, h, w, c)
        """
        b = int(windows.shape[0] / (h * w / window_size / window_size))
        x = windows.view(b, h // window_size, w // window_size, window_size, window_size, -1)
        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)
        return x

    class WindowAttention_hat(nn.Module):
        r""" Window based multi-head self attention (W-MSA) module with relative position bias.
        It supports both of shifted and non-shifted window.

        Args:
            dim (int): Number of input channels.
            window_size (tuple[int]): The height and width of the window.
            num_heads (int): Number of attention heads.
            qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
            attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
            proj_drop (float, optional): Dropout ratio of output. Default: 0.0
        """

        def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

            super().__init__()
            self.dim = dim
            self.window_size = window_size  # Wh, Ww
            self.num_heads = num_heads
            head_dim = dim // num_heads
            self.scale = qk_scale or head_dim ** -0.5

            # define a parameter table of relative position bias
            self.relative_position_bias_table = nn.Parameter(
                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
            self.attn_drop = nn.Dropout(attn_drop)
            self.proj = nn.Linear(dim, dim)

            self.proj_drop = nn.Dropout(proj_drop)

            trunc_normal_(self.relative_position_bias_table, std=.02)
            self.softmax = nn.Softmax(dim=-1)

        def forward(self, x, rpi, mask=None):
            """
            Args:
                x: input features with shape of (num_windows*b, n, c)
                mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
            """
            b_, n, c = x.shape
            qkv = self.qkv(x).reshape(b_, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

            q = q * self.scale
            attn = (q @ k.transpose(-2, -1))

            relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(
                self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
            attn = attn + relative_position_bias.unsqueeze(0)

            if mask is not None:
                nw = mask.shape[0]
                attn = attn.view(b_ // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)
                attn = attn.view(-1, self.num_heads, n, n)
                attn = self.softmax(attn)
            else:
                attn = self.softmax(attn)

            attn = self.attn_drop(attn)

            x = (attn @ v).transpose(1, 2).reshape(b_, n, c)
            x = self.proj(x)
            x = self.proj_drop(x)
            return x

    class WindowAttention(nn.Module):
        r""" Window based multi-head self attention (W-MSA) module with relative position bias.
        It supports both of shifted and non-shifted window.

        Args:
            dim (int): Number of input channels.
            window_size (tuple[int]): The height and width of the window.
            num_heads (int): Number of attention heads.
            qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
            attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
            proj_drop (float, optional): Dropout ratio of output. Default: 0.0
        """

        def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

            super().__init__()
            self.dim = dim
            self.window_size = window_size  # Wh, Ww
            self.num_heads = num_heads
            head_dim = dim // num_heads
            self.scale = qk_scale or head_dim ** -0.5

            # define a parameter table of relative position bias
            self.relative_position_bias_table = nn.Parameter(
                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

            # get pair-wise relative position index for each token inside the window
            coords_h = torch.arange(self.window_size[0])
            coords_w = torch.arange(self.window_size[1])
            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
            self.register_buffer("relative_position_index", relative_position_index)

            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
            self.attn_drop = nn.Dropout(attn_drop)
            self.proj = nn.Linear(dim, dim)

            self.proj_drop = nn.Dropout(proj_drop)

            trunc_normal_(self.relative_position_bias_table, std=.02)
            self.softmax = nn.Softmax(dim=-1)

        def forward(self, x, mask=None):
            """
            Args:
                x: input features with shape of (num_windows*B, N, C)
                mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
            """
            B_, N, C = x.shape
            qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

            q = q * self.scale
            attn = (q @ k.transpose(-2, -1))

            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
                self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
            attn = attn + relative_position_bias.unsqueeze(0)

            if mask is not None:
                nW = mask.shape[0]
                attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
                attn = attn.view(-1, self.num_heads, N, N)
                attn = self.softmax(attn)
            else:
                attn = self.softmax(attn)

            attn = self.attn_drop(attn)

            x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
            x = self.proj(x)
            x = self.proj_drop(x)
            return x

        def extra_repr(self) -> str:
            return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

        def flops(self, N):
            # calculate flops for 1 window with token length of N
            flops = 0
            # qkv = self.qkv(x)
            flops += N * self.dim * 3 * self.dim
            # attn = (q @ k.transpose(-2, -1))
            flops += self.num_heads * N * (self.dim // self.num_heads) * N
            #  x = (attn @ v)
            flops += self.num_heads * N * N * (self.dim // self.num_heads)
            # x = self.proj(x)
            flops += N * self.dim * self.dim
            return flops

    class HAB(nn.Module):
        r""" Hybrid Attention Block.

        Args:
            dim (int): Number of input channels.
            input_resolution (tuple[int]): Input resolution.
            num_heads (int): Number of attention heads.
            window_size (int): Window size.
            shift_size (int): Shift size for SW-MSA.
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
            drop (float, optional): Dropout rate. Default: 0.0
            attn_drop (float, optional): Attention dropout rate. Default: 0.0
            drop_path (float, optional): Stochastic depth rate. Default: 0.0
            act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        """

        def __init__(self,
                     dim,
                     input_resolution,
                     num_heads,
                     window_size=7,
                     shift_size=0,
                     compress_ratio=3,
                     squeeze_factor=30,
                     conv_scale=0.01,
                     mlp_ratio=4.,
                     qkv_bias=True,
                     qk_scale=None,
                     drop=0.,
                     attn_drop=0.,
                     drop_path=0.,
                     act_layer=nn.GELU,
                     norm_layer=nn.LayerNorm):
            super().__init__()
            self.dim = dim
            self.input_resolution = input_resolution
            self.num_heads = num_heads
            self.window_size = window_size
            self.shift_size = shift_size
            self.mlp_ratio = mlp_ratio
            if min(self.input_resolution) <= self.window_size:
                # if window size is larger than input resolution, we don't partition windows
                self.shift_size = 0
                self.window_size = min(self.input_resolution)
            assert 0 <= self.shift_size < self.window_size, 'shift_size must in 0-window_size'

            self.norm1 = norm_layer(dim)
            self.attn = WindowAttention_hat(
                dim,
                window_size=to_2tuple(self.window_size),
                num_heads=num_heads,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                attn_drop=attn_drop,
                proj_drop=drop)

            self.conv_scale = conv_scale
            self.conv_block = CAB(num_feat=dim, compress_ratio=compress_ratio, squeeze_factor=squeeze_factor)

            self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
            self.norm2 = norm_layer(dim)
            mlp_hidden_dim = int(dim * mlp_ratio)
            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        def forward(self, x, x_size, rpi_sa, attn_mask):
            h, w = x_size
            b, _, c = x.shape
            # assert seq_len == h * w, "input feature has wrong size"

            shortcut = x
            x = self.norm1(x)
            x = x.view(b, h, w, c)

            # Conv_X
            conv_x = self.conv_block(x.permute(0, 3, 1, 2))
            conv_x = conv_x.permute(0, 2, 3, 1).contiguous().view(b, h * w, c)

            # cyclic shift
            if self.shift_size > 0:
                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
                attn_mask = attn_mask
            else:
                shifted_x = x
                attn_mask = None

            # partition windows
            x_windows = window_partition(shifted_x, self.window_size)  # nw*b, window_size, window_size, c
            x_windows = x_windows.view(-1, self.window_size * self.window_size, c)  # nw*b, window_size*window_size, c

            # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size
            attn_windows = self.attn(x_windows, rpi=rpi_sa, mask=attn_mask)

            # merge windows
            attn_windows = attn_windows.view(-1, self.window_size, self.window_size, c)
            shifted_x = window_reverse(attn_windows, self.window_size, h, w)  # b h' w' c

            # reverse cyclic shift
            if self.shift_size > 0:
                attn_x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
            else:
                attn_x = shifted_x
            attn_x = attn_x.view(b, h * w, c)

            # FFN
            x = shortcut + self.drop_path(attn_x) + conv_x * self.conv_scale
            x = x + self.drop_path(self.mlp(self.norm2(x)))

            return x

    class PatchMerging(nn.Module):
        r""" Patch Merging Layer.

        Args:
            input_resolution (tuple[int]): Resolution of input feature.
            dim (int): Number of input channels.
            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        """

        def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
            super().__init__()
            self.input_resolution = input_resolution
            self.dim = dim
            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
            self.norm = norm_layer(4 * dim)

        def forward(self, x):
            """
            x: b, h*w, c
            """
            h, w = self.input_resolution
            b, seq_len, c = x.shape
            assert seq_len == h * w, 'input feature has wrong size'
            assert h % 2 == 0 and w % 2 == 0, f'x size ({h}*{w}) are not even.'

            x = x.view(b, h, w, c)

            x0 = x[:, 0::2, 0::2, :]  # b h/2 w/2 c
            x1 = x[:, 1::2, 0::2, :]  # b h/2 w/2 c
            x2 = x[:, 0::2, 1::2, :]  # b h/2 w/2 c
            x3 = x[:, 1::2, 1::2, :]  # b h/2 w/2 c
            x = torch.cat([x0, x1, x2, x3], -1)  # b h/2 w/2 4*c
            x = x.view(b, -1, 4 * c)  # b h/2*w/2 4*c

            x = self.norm(x)
            x = self.reduction(x)

            return x

    class OCAB(nn.Module):
        # overlapping cross-attention block

        def __init__(self, dim,
                     input_resolution,
                     window_size,
                     overlap_ratio,
                     num_heads,
                     qkv_bias=True,
                     qk_scale=None,
                     mlp_ratio=2,
                     norm_layer=nn.LayerNorm
                     ):
            super().__init__()
            self.dim = dim
            self.input_resolution = input_resolution
            self.window_size = window_size
            self.num_heads = num_heads
            head_dim = dim // num_heads
            self.scale = qk_scale or head_dim ** -0.5
            self.overlap_win_size = int(window_size * overlap_ratio) + window_size

            self.norm1 = norm_layer(dim)
            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
            self.unfold = nn.Unfold(kernel_size=(self.overlap_win_size, self.overlap_win_size), stride=window_size, padding=(self.overlap_win_size - window_size) // 2)

            # define a parameter table of relative position bias
            self.relative_position_bias_table = nn.Parameter(
                torch.zeros((window_size + self.overlap_win_size - 1) * (window_size + self.overlap_win_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

            trunc_normal_(self.relative_position_bias_table, std=.02)
            self.softmax = nn.Softmax(dim=-1)

            self.proj = nn.Linear(dim, dim)

            self.norm2 = norm_layer(dim)
            mlp_hidden_dim = int(dim * mlp_ratio)
            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU)

        def forward(self, x, x_size, rpi):
            h, w = x_size
            b, _, c = x.shape

            shortcut = x
            x = self.norm1(x)
            x = x.view(b, h, w, c)

            qkv = self.qkv(x).reshape(b, h, w, 3, c).permute(3, 0, 4, 1, 2)  # 3, b, c, h, w
            q = qkv[0].permute(0, 2, 3, 1)  # b, h, w, c
            kv = torch.cat((qkv[1], qkv[2]), dim=1)  # b, 2*c, h, w

            # partition windows
            q_windows = window_partition(q, self.window_size)  # nw*b, window_size, window_size, c
            q_windows = q_windows.view(-1, self.window_size * self.window_size, c)  # nw*b, window_size*window_size, c

            kv_windows = self.unfold(kv)  # b, c*w*w, nw
            kv_windows = rearrange(kv_windows, 'b (nc ch owh oww) nw -> nc (b nw) (owh oww) ch', nc=2, ch=c, owh=self.overlap_win_size, oww=self.overlap_win_size).contiguous()  # 2, nw*b, ow*ow, c
            k_windows, v_windows = kv_windows[0], kv_windows[1]  # nw*b, ow*ow, c

            b_, nq, _ = q_windows.shape
            _, n, _ = k_windows.shape
            d = self.dim // self.num_heads
            q = q_windows.reshape(b_, nq, self.num_heads, d).permute(0, 2, 1, 3)  # nw*b, nH, nq, d
            k = k_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3)  # nw*b, nH, n, d
            v = v_windows.reshape(b_, n, self.num_heads, d).permute(0, 2, 1, 3)  # nw*b, nH, n, d

            q = q * self.scale
            attn = (q @ k.transpose(-2, -1))

            relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(
                self.window_size * self.window_size, self.overlap_win_size * self.overlap_win_size, -1)  # ws*ws, wse*wse, nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, ws*ws, wse*wse
            attn = attn + relative_position_bias.unsqueeze(0)

            attn = self.softmax(attn)
            attn_windows = (attn @ v).transpose(1, 2).reshape(b_, nq, self.dim)

            # merge windows
            attn_windows = attn_windows.view(-1, self.window_size, self.window_size, self.dim)
            x = window_reverse(attn_windows, self.window_size, h, w)  # b h w c
            x = x.view(b, h * w, self.dim)

            x = self.proj(x) + shortcut

            x = x + self.mlp(self.norm2(x))
            return x

    class AttenBlocks(nn.Module):
        """ A series of attention blocks for one RHAG.

        Args:
            dim (int): Number of input channels.
            input_resolution (tuple[int]): Input resolution.
            depth (int): Number of blocks.
            num_heads (int): Number of attention heads.
            window_size (int): Local window size.
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
            drop (float, optional): Dropout rate. Default: 0.0
            attn_drop (float, optional): Attention dropout rate. Default: 0.0
            drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
            norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
            downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
        """

        def __init__(self,
                     dim,
                     input_resolution,
                     depth,
                     num_heads,
                     window_size,
                     compress_ratio,
                     squeeze_factor,
                     conv_scale,
                     overlap_ratio,
                     mlp_ratio=4.,
                     qkv_bias=True,
                     qk_scale=None,
                     drop=0.,
                     attn_drop=0.,
                     drop_path=0.,
                     norm_layer=nn.LayerNorm,
                     downsample=None,
                     use_checkpoint=False):

            super().__init__()
            self.dim = dim
            self.input_resolution = input_resolution
            self.depth = depth
            self.use_checkpoint = use_checkpoint

            # build blocks
            self.blocks = nn.ModuleList([
                HAB(
                    dim=dim,
                    input_resolution=input_resolution,
                    num_heads=num_heads,
                    window_size=window_size,
                    shift_size=0 if (i % 2 == 0) else window_size // 2,
                    compress_ratio=compress_ratio,
                    squeeze_factor=squeeze_factor,
                    conv_scale=conv_scale,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    drop=drop,
                    attn_drop=attn_drop,
                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                    norm_layer=norm_layer) for i in range(depth)
            ])

            # OCAB
            self.overlap_attn = OCAB(
                dim=dim,
                input_resolution=input_resolution,
                window_size=window_size,
                overlap_ratio=overlap_ratio,
                num_heads=num_heads,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                mlp_ratio=mlp_ratio,
                norm_layer=norm_layer
            )

            # patch merging layer
            if downsample is not None:
                self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
            else:
                self.downsample = None

        def forward(self, x, x_size, params):
            for blk in self.blocks:
                x = blk(x, x_size, params['rpi_sa'], params['attn_mask'])

            x = self.overlap_attn(x, x_size, params['rpi_oca'])

            if self.downsample is not None:
                x = self.downsample(x)
            return x

    class RHAG(nn.Module):
        """Residual Hybrid Attention Group (RHAG).

        Args:
            dim (int): Number of input channels.
            input_resolution (tuple[int]): Input resolution.
            depth (int): Number of blocks.
            num_heads (int): Number of attention heads.
            window_size (int): Local window size.
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
            drop (float, optional): Dropout rate. Default: 0.0
            attn_drop (float, optional): Attention dropout rate. Default: 0.0
            drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
            norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
            downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
            img_size: Input image size.
            patch_size: Patch size.
            resi_connection: The convolutional block before residual connection.
        """

        def __init__(self,
                     dim,
                     input_resolution,
                     depth,
                     num_heads,
                     window_size,
                     compress_ratio,
                     squeeze_factor,
                     conv_scale,
                     overlap_ratio,
                     mlp_ratio=4.,
                     qkv_bias=True,
                     qk_scale=None,
                     drop=0.,
                     attn_drop=0.,
                     drop_path=0.,
                     norm_layer=nn.LayerNorm,
                     downsample=None,
                     use_checkpoint=False,
                     img_size=224,
                     patch_size=4,
                     resi_connection='1conv'):
            super(RHAG, self).__init__()

            self.dim = dim
            self.input_resolution = input_resolution

            self.residual_group = AttenBlocks(
                dim=dim,
                input_resolution=input_resolution,
                depth=depth,
                num_heads=num_heads,
                window_size=window_size,
                compress_ratio=compress_ratio,
                squeeze_factor=squeeze_factor,
                conv_scale=conv_scale,
                overlap_ratio=overlap_ratio,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop,
                attn_drop=attn_drop,
                drop_path=drop_path,
                norm_layer=norm_layer,
                downsample=downsample,
                use_checkpoint=use_checkpoint)

            if resi_connection == '1conv':
                self.conv = nn.Conv2d(dim, dim, 3, 1, 1)
            elif resi_connection == 'identity':
                self.conv = nn.Identity()

            self.patch_embed = PatchEmbed(
                img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim, norm_layer=None)

            self.patch_unembed = PatchUnEmbed(
                img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim, norm_layer=None)

        def forward(self, x, x_size, params):
            return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size, params), x_size))) + x

    class PatchEmbed(nn.Module):
        r""" Image to Patch Embedding

        Args:
            img_size (int): Image size.  Default: 224.
            patch_size (int): Patch token size. Default: 4.
            in_chans (int): Number of input image channels. Default: 3.
            embed_dim (int): Number of linear projection output channels. Default: 96.
            norm_layer (nn.Module, optional): Normalization layer. Default: None
        """

        def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
            super().__init__()
            img_size = to_2tuple(img_size)
            patch_size = to_2tuple(patch_size)
            patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
            self.img_size = img_size
            self.patch_size = patch_size
            self.patches_resolution = patches_resolution
            self.num_patches = patches_resolution[0] * patches_resolution[1]

            self.in_chans = in_chans
            self.embed_dim = embed_dim

            if norm_layer is not None:
                self.norm = norm_layer(embed_dim)
            else:
                self.norm = None

        def forward(self, x):
            x = x.flatten(2).transpose(1, 2)  # b Ph*Pw c
            if self.norm is not None:
                x = self.norm(x)
            return x

    class PatchUnEmbed(nn.Module):
        r""" Image to Patch Unembedding

        Args:
            img_size (int): Image size.  Default: 224.
            patch_size (int): Patch token size. Default: 4.
            in_chans (int): Number of input image channels. Default: 3.
            embed_dim (int): Number of linear projection output channels. Default: 96.
            norm_layer (nn.Module, optional): Normalization layer. Default: None
        """

        def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
            super().__init__()
            img_size = to_2tuple(img_size)
            patch_size = to_2tuple(patch_size)
            patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
            self.img_size = img_size
            self.patch_size = patch_size
            self.patches_resolution = patches_resolution
            self.num_patches = patches_resolution[0] * patches_resolution[1]

            self.in_chans = in_chans
            self.embed_dim = embed_dim

        def forward(self, x, x_size):
            x = x.transpose(1, 2).contiguous().view(x.shape[0], self.embed_dim, x_size[0], x_size[1])  # b Ph*Pw c
            return x

    class Upsample(nn.Sequential):
        """Upsample module.

        Args:
            scale (int): Scale factor. Supported scales: 2^n and 3.
            num_feat (int): Channel number of intermediate features.
        """

        def __init__(self, scale, num_feat):
            m = []
            if (scale & (scale - 1)) == 0:  # scale = 2^n
                for _ in range(int(math.log(scale, 2))):
                    m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))
                    m.append(nn.PixelShuffle(2))
            elif scale == 3:
                m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))
                m.append(nn.PixelShuffle(3))
            else:
                raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')
            super(Upsample, self).__init__(*m)

    class HAT(nn.Module):
        r""" Hybrid Attention Transformer
            A PyTorch implementation of : `Activating More Pixels in Image Super-Resolution Transformer`.
            Some codes are based on SwinIR.
        Args:
            img_size (int | tuple(int)): Input image size. Default 64
            patch_size (int | tuple(int)): Patch size. Default: 1
            in_chans (int): Number of input image channels. Default: 3
            embed_dim (int): Patch embedding dimension. Default: 96
            depths (tuple(int)): Depth of each Swin Transformer layer.
            num_heads (tuple(int)): Number of attention heads in different layers.
            window_size (int): Window size. Default: 7
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
            qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
            drop_rate (float): Dropout rate. Default: 0
            attn_drop_rate (float): Attention dropout rate. Default: 0
            drop_path_rate (float): Stochastic depth rate. Default: 0.1
            norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
            ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
            patch_norm (bool): If True, add normalization after patch embedding. Default: True
            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
            upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction
            img_range: Image range. 1. or 255.
            upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None
            resi_connection: The convolutional block before residual connection. '1conv'/'3conv'
        """

        def __init__(self,
                     img_size=64,
                     patch_size=1,
                     in_chans=3,
                     embed_dim=180,
                     depths=(6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6),
                     num_heads=(6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6),
                     window_size=16,
                     compress_ratio=3,
                     squeeze_factor=30,
                     conv_scale=0.01,
                     overlap_ratio=0.5,
                     mlp_ratio=2.,
                     qkv_bias=True,
                     qk_scale=None,
                     drop_rate=0.,
                     attn_drop_rate=0.,
                     drop_path_rate=0.1,
                     norm_layer=nn.LayerNorm,
                     ape=False,
                     patch_norm=True,
                     use_checkpoint=False,
                     upscale=2,
                     img_range=1.,
                     upsampler='',
                     resi_connection='1conv',
                     **kwargs):
            super(HAT, self).__init__()

            self.window_size = window_size
            self.shift_size = window_size // 2
            self.overlap_ratio = overlap_ratio

            num_in_ch = in_chans
            num_out_ch = in_chans
            num_feat = 64
            self.img_range = img_range
            if in_chans == 3:
                rgb_mean = (0.4488, 0.4371, 0.4040)
                self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)
            else:
                self.mean = torch.zeros(1, 1, 1, 1)
            self.upscale = upscale
            self.upsampler = upsampler

            # relative position index
            relative_position_index_SA = self.calculate_rpi_sa()
            relative_position_index_OCA = self.calculate_rpi_oca()
            self.register_buffer('relative_position_index_SA', relative_position_index_SA)
            self.register_buffer('relative_position_index_OCA', relative_position_index_OCA)

            # ------------------------- 1, shallow feature extraction ------------------------- #
            self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)

            # ------------------------- 2, deep feature extraction ------------------------- #
            self.num_layers = len(depths)
            self.embed_dim = embed_dim
            self.ape = ape
            self.patch_norm = patch_norm
            self.num_features = embed_dim
            self.mlp_ratio = mlp_ratio

            # split image into non-overlapping patches
            self.patch_embed = PatchEmbed(
                img_size=img_size,
                patch_size=patch_size,
                in_chans=embed_dim,
                embed_dim=embed_dim,
                norm_layer=norm_layer if self.patch_norm else None)
            num_patches = self.patch_embed.num_patches
            patches_resolution = self.patch_embed.patches_resolution
            self.patches_resolution = patches_resolution

            # merge non-overlapping patches into image
            self.patch_unembed = PatchUnEmbed(
                img_size=img_size,
                patch_size=patch_size,
                in_chans=embed_dim,
                embed_dim=embed_dim,
                norm_layer=norm_layer if self.patch_norm else None)

            # absolute position embedding
            if self.ape:
                self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
                trunc_normal_(self.absolute_pos_embed, std=.02)

            self.pos_drop = nn.Dropout(p=drop_rate)

            # stochastic depth
            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

            # build Residual Hybrid Attention Groups (RHAG)
            self.layers = nn.ModuleList()
            for i_layer in range(self.num_layers):
                layer = RHAG(
                    dim=embed_dim,
                    input_resolution=(patches_resolution[0], patches_resolution[1]),
                    depth=depths[i_layer],
                    num_heads=num_heads[i_layer],
                    window_size=window_size,
                    compress_ratio=compress_ratio,
                    squeeze_factor=squeeze_factor,
                    conv_scale=conv_scale,
                    overlap_ratio=overlap_ratio,
                    mlp_ratio=self.mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    drop=drop_rate,
                    attn_drop=attn_drop_rate,
                    drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results
                    norm_layer=norm_layer,
                    downsample=None,
                    use_checkpoint=use_checkpoint,
                    img_size=img_size,
                    patch_size=patch_size,
                    resi_connection=resi_connection)
                self.layers.append(layer)
            self.norm = norm_layer(self.num_features)

            # build the last conv layer in deep feature extraction
            if resi_connection == '1conv':
                self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)
            elif resi_connection == 'identity':
                self.conv_after_body = nn.Identity()

            # ------------------------- 3, high quality image reconstruction ------------------------- #
            if self.upsampler == 'pixelshuffle':
                # for classical SR
                self.conv_before_upsample = nn.Sequential(
                    nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True))
                self.upsample = Upsample(upscale, num_feat)
                self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)
            else:
                self.conv_last = nn.Conv2d(self.num_features, num_out_ch, 3, 1, 1)

            self.apply(self._init_weights)

        def _init_weights(self, m):
            if isinstance(m, nn.Linear):
                trunc_normal_(m.weight, std=.02)
                if isinstance(m, nn.Linear) and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

        def calculate_rpi_sa(self):
            # calculate relative position index for SA
            coords_h = torch.arange(self.window_size)
            coords_w = torch.arange(self.window_size)
            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
            relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.window_size - 1
            relative_coords[:, :, 0] *= 2 * self.window_size - 1
            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
            return relative_position_index

        def calculate_rpi_oca(self):
            # calculate relative position index for OCA
            window_size_ori = self.window_size
            window_size_ext = self.window_size + int(self.overlap_ratio * self.window_size)

            coords_h = torch.arange(window_size_ori)
            coords_w = torch.arange(window_size_ori)
            coords_ori = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, ws, ws
            coords_ori_flatten = torch.flatten(coords_ori, 1)  # 2, ws*ws

            coords_h = torch.arange(window_size_ext)
            coords_w = torch.arange(window_size_ext)
            coords_ext = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, wse, wse
            coords_ext_flatten = torch.flatten(coords_ext, 1)  # 2, wse*wse

            relative_coords = coords_ext_flatten[:, None, :] - coords_ori_flatten[:, :, None]  # 2, ws*ws, wse*wse

            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # ws*ws, wse*wse, 2
            relative_coords[:, :, 0] += window_size_ori - window_size_ext + 1  # shift to start from 0
            relative_coords[:, :, 1] += window_size_ori - window_size_ext + 1

            relative_coords[:, :, 0] *= window_size_ori + window_size_ext - 1
            relative_position_index = relative_coords.sum(-1)
            return relative_position_index

        def calculate_mask(self, x_size):
            # calculate attention mask for SW-MSA
            h, w = x_size
            img_mask = torch.zeros((1, h, w, 1))  # 1 h w 1
            h_slices = (slice(0, -self.window_size), slice(-self.window_size,
                                                           -self.shift_size), slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size), slice(-self.window_size,
                                                           -self.shift_size), slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nw, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))

            return attn_mask

        @torch.jit.ignore
        def no_weight_decay(self):
            return {'absolute_pos_embed'}

        @torch.jit.ignore
        def no_weight_decay_keywords(self):
            return {'relative_position_bias_table'}

        def forward_features(self, x):
            x_size = (x.shape[2], x.shape[3])

            # Calculate attention mask and relative position index in advance to speed up inference.
            # The original code is very time-cosuming for large window size.
            attn_mask = self.calculate_mask(x_size).to(x.device)
            params = {'attn_mask': attn_mask, 'rpi_sa': self.relative_position_index_SA, 'rpi_oca': self.relative_position_index_OCA}

            x = self.patch_embed(x)
            if self.ape:
                x = x + self.absolute_pos_embed
            x = self.pos_drop(x)

            for layer in self.layers:
                x = layer(x, x_size, params)

            x = self.norm(x)  # b seq_len c
            x = self.patch_unembed(x, x_size)

            return x

        def forward(self, x):
            x = self.conv_first(x)
            x = self.conv_after_body(self.forward_features(x)) + x
            x = self.conv_last(x)

            return x

    return HAT


def get_naf():
    ##############################################################
    ################            NAF NET              #############
    ##############################################################
    '''
    Simple Baselines for Image Restoration

    @article{chen2022simple,
      title={Simple Baselines for Image Restoration},
      author={Chen, Liangyu and Chu, Xiaojie and Zhang, Xiangyu and Sun, Jian},
      journal={arXiv preprint arXiv:2204.04676},
      year={2022}
    }
    '''

    class LayerNormFunction(torch.autograd.Function):
        @staticmethod
        def forward(ctx, x, weight, bias, eps):
            ctx.eps = eps
            N, C, H, W = x.size()
            mu = x.mean(1, keepdim=True)
            var = (x - mu).pow(2).mean(1, keepdim=True)
            y = (x - mu) / (var + eps).sqrt()
            ctx.save_for_backward(y, var, weight)
            y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)
            return y

        @staticmethod
        def backward(ctx, grad_output):
            eps = ctx.eps

            N, C, H, W = grad_output.size()
            y, var, weight = ctx.saved_variables
            g = grad_output * weight.view(1, C, 1, 1)
            mean_g = g.mean(dim=1, keepdim=True)

            mean_gy = (g * y).mean(dim=1, keepdim=True)
            gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)
            return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(
                dim=0), None

    class LayerNorm2d(nn.Module):
        def __init__(self, channels, eps=1e-6):
            super(LayerNorm2d, self).__init__()
            self.register_parameter('weight', nn.Parameter(torch.ones(channels)))
            self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))
            self.eps = eps

        def forward(self, x):
            return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)

    class SimpleGate(nn.Module):
        def forward(self, x):
            x1, x2 = x.chunk(2, dim=1)
            return x1 * x2

    class NAFBlock(nn.Module):
        def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.):
            super().__init__()
            dw_channel = c * DW_Expand
            self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)
            self.conv2 = nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel,
                                   bias=True)
            self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)

            # Simplified Channel Attention
            self.sca = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1,
                          groups=1, bias=True),
            )

            # SimpleGate
            self.sg = SimpleGate()

            ffn_channel = FFN_Expand * c
            self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)
            self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)

            self.norm1 = LayerNorm2d(c)
            self.norm2 = LayerNorm2d(c)

            self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()
            self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()

            self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)
            self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)

        def forward(self, inp):
            x = inp

            x = self.norm1(x)

            x = self.conv1(x)
            x = self.conv2(x)
            x = self.sg(x)
            x = x * self.sca(x)
            x = self.conv3(x)

            x = self.dropout1(x)

            y = inp + x * self.beta

            x = self.conv4(self.norm2(y))
            x = self.sg(x)
            x = self.conv5(x)

            x = self.dropout2(x)

            return y + x * self.gamma

    class NAFNet(nn.Module):
        def __init__(self, img_channel=3, width=64, middle_blk_num=12, enc_blk_nums=[2, 2, 4, 8], dec_blk_nums=[2, 2, 2, 2]):
            super().__init__()

            self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,
                                   bias=True)
            self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,
                                    bias=True)

            self.encoders = nn.ModuleList()
            self.decoders = nn.ModuleList()
            self.middle_blks = nn.ModuleList()
            self.ups = nn.ModuleList()
            self.downs = nn.ModuleList()

            chan = width
            for num in enc_blk_nums:
                self.encoders.append(
                    nn.Sequential(
                        *[NAFBlock(chan) for _ in range(num)]
                    )
                )
                self.downs.append(
                    nn.Conv2d(chan, 2 * chan, 2, 2)
                )
                chan = chan * 2

            self.middle_blks = \
                nn.Sequential(
                    *[NAFBlock(chan) for _ in range(middle_blk_num)]
                )

            for num in dec_blk_nums:
                self.ups.append(
                    nn.Sequential(
                        nn.Conv2d(chan, chan * 2, 1, bias=False),
                        nn.PixelShuffle(2)
                    )
                )
                chan = chan // 2
                self.decoders.append(
                    nn.Sequential(
                        *[NAFBlock(chan) for _ in range(num)]
                    )
                )

            self.padder_size = 2 ** len(self.encoders)

        def forward(self, inp):
            B, C, H, W = inp.shape
            inp = self.check_image_size(inp)

            x = self.intro(inp)

            encs = []

            for encoder, down in zip(self.encoders, self.downs):
                x = encoder(x)
                encs.append(x)
                x = down(x)

            x = self.middle_blks(x)

            for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):
                x = up(x)
                x = x + enc_skip
                x = decoder(x)

            x = self.ending(x)
            x = x + inp

            return x[:, :, :H, :W]

        def check_image_size(self, x):
            _, _, h, w = x.size()
            mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size
            mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size
            x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))
            return x

    return NAFNet


def get_swinir():
    ##############################################################
    ################            SWIN IR              #############
    ##############################################################
    # -----------------------------------------------------------------------------------
    # SwinIR: Image Restoration Using Swin Transformer, https://arxiv.org/abs/2108.10257
    # Originally Written by Ze Liu, Modified by Jingyun Liang.
    # -----------------------------------------------------------------------------------
    import math
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.utils.checkpoint as checkpoint
    from timm.models.layers import DropPath, to_2tuple, trunc_normal_

    class Mlp(nn.Module):
        def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
            super().__init__()
            out_features = out_features or in_features
            hidden_features = hidden_features or in_features
            self.fc1 = nn.Linear(in_features, hidden_features)
            self.act = act_layer()
            self.fc2 = nn.Linear(hidden_features, out_features)
            self.drop = nn.Dropout(drop)

        def forward(self, x):
            x = self.fc1(x)
            x = self.act(x)
            x = self.drop(x)
            x = self.fc2(x)
            x = self.drop(x)
            return x

    def window_partition(x, window_size):
        """
        Args:
            x: (B, H, W, C)
            window_size (int): window size

        Returns:
            windows: (num_windows*B, window_size, window_size, C)
        """
        B, H, W, C = x.shape
        x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
        return windows

    def window_reverse(windows, window_size, H, W):
        """
        Args:
            windows: (num_windows*B, window_size, window_size, C)
            window_size (int): Window size
            H (int): Height of image
            W (int): Width of image

        Returns:
            x: (B, H, W, C)
        """
        B = int(windows.shape[0] / (H * W / window_size / window_size))
        x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
        return x

    class WindowAttention(nn.Module):
        r""" Window based multi-head self attention (W-MSA) module with relative position bias.
        It supports both of shifted and non-shifted window.

        Args:
            dim (int): Number of input channels.
            window_size (tuple[int]): The height and width of the window.
            num_heads (int): Number of attention heads.
            qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
            attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
            proj_drop (float, optional): Dropout ratio of output. Default: 0.0
        """

        def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

            super().__init__()
            self.dim = dim
            self.window_size = window_size  # Wh, Ww
            self.num_heads = num_heads
            head_dim = dim // num_heads
            self.scale = qk_scale or head_dim ** -0.5

            # define a parameter table of relative position bias
            self.relative_position_bias_table = nn.Parameter(
                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

            # get pair-wise relative position index for each token inside the window
            coords_h = torch.arange(self.window_size[0])
            coords_w = torch.arange(self.window_size[1])
            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
            relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
            self.register_buffer("relative_position_index", relative_position_index)

            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
            self.attn_drop = nn.Dropout(attn_drop)
            self.proj = nn.Linear(dim, dim)

            self.proj_drop = nn.Dropout(proj_drop)

            trunc_normal_(self.relative_position_bias_table, std=.02)
            self.softmax = nn.Softmax(dim=-1)

        def forward(self, x, mask=None):
            """
            Args:
                x: input features with shape of (num_windows*B, N, C)
                mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
            """
            B_, N, C = x.shape
            qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

            q = q * self.scale
            attn = (q @ k.transpose(-2, -1))

            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
                self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
            attn = attn + relative_position_bias.unsqueeze(0)

            if mask is not None:
                nW = mask.shape[0]
                attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
                attn = attn.view(-1, self.num_heads, N, N)
                attn = self.softmax(attn)
            else:
                attn = self.softmax(attn)

            attn = self.attn_drop(attn)

            x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
            x = self.proj(x)
            x = self.proj_drop(x)
            return x

        def extra_repr(self) -> str:
            return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

        def flops(self, N):
            # calculate flops for 1 window with token length of N
            flops = 0
            # qkv = self.qkv(x)
            flops += N * self.dim * 3 * self.dim
            # attn = (q @ k.transpose(-2, -1))
            flops += self.num_heads * N * (self.dim // self.num_heads) * N
            #  x = (attn @ v)
            flops += self.num_heads * N * N * (self.dim // self.num_heads)
            # x = self.proj(x)
            flops += N * self.dim * self.dim
            return flops

    class SwinTransformerBlock(nn.Module):
        r""" Swin Transformer Block.

        Args:
            dim (int): Number of input channels.
            input_resolution (tuple[int]): Input resulotion.
            num_heads (int): Number of attention heads.
            window_size (int): Window size.
            shift_size (int): Shift size for SW-MSA.
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
            drop (float, optional): Dropout rate. Default: 0.0
            attn_drop (float, optional): Attention dropout rate. Default: 0.0
            drop_path (float, optional): Stochastic depth rate. Default: 0.0
            act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        """

        def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                     mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                     act_layer=nn.GELU, norm_layer=nn.LayerNorm):
            super().__init__()
            self.dim = dim
            self.input_resolution = input_resolution
            self.num_heads = num_heads
            self.window_size = window_size
            self.shift_size = shift_size
            self.mlp_ratio = mlp_ratio
            if min(self.input_resolution) <= self.window_size:
                # if window size is larger than input resolution, we don't partition windows
                self.shift_size = 0
                self.window_size = min(self.input_resolution)
            assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

            self.norm1 = norm_layer(dim)
            self.attn = WindowAttention(
                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

            self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
            self.norm2 = norm_layer(dim)
            mlp_hidden_dim = int(dim * mlp_ratio)
            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

            if self.shift_size > 0:
                attn_mask = self.calculate_mask(self.input_resolution)
            else:
                attn_mask = None

            self.register_buffer("attn_mask", attn_mask)

        def calculate_mask(self, x_size):
            # calculate attention mask for SW-MSA
            H, W = x_size
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))

            return attn_mask

        def forward(self, x, x_size):
            H, W = x_size
            B, L, C = x.shape
            # assert L == H * W, "input feature has wrong size"

            shortcut = x
            x = self.norm1(x)
            x = x.view(B, H, W, C)

            # cyclic shift
            if self.shift_size > 0:
                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            else:
                shifted_x = x

            # partition windows
            x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
            x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

            # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size
            if self.input_resolution == x_size:
                attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C
            else:
                attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))

            # merge windows
            attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
            shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C

            # reverse cyclic shift
            if self.shift_size > 0:
                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
            else:
                x = shifted_x
            x = x.view(B, H * W, C)

            # FFN
            x = shortcut + self.drop_path(x)
            x = x + self.drop_path(self.mlp(self.norm2(x)))

            return x

        def extra_repr(self) -> str:
            return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
                   f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

        def flops(self):
            flops = 0
            H, W = self.input_resolution
            # norm1
            flops += self.dim * H * W
            # W-MSA/SW-MSA
            nW = H * W / self.window_size / self.window_size
            flops += nW * self.attn.flops(self.window_size * self.window_size)
            # mlp
            flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
            # norm2
            flops += self.dim * H * W
            return flops

    class PatchMerging(nn.Module):
        r""" Patch Merging Layer.

        Args:
            input_resolution (tuple[int]): Resolution of input feature.
            dim (int): Number of input channels.
            norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        """

        def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
            super().__init__()
            self.input_resolution = input_resolution
            self.dim = dim
            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
            self.norm = norm_layer(4 * dim)

        def forward(self, x):
            """
            x: B, H*W, C
            """
            H, W = self.input_resolution
            B, L, C = x.shape
            assert L == H * W, "input feature has wrong size"
            assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

            x = x.view(B, H, W, C)

            x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
            x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
            x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
            x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
            x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
            x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

            x = self.norm(x)
            x = self.reduction(x)

            return x

        def extra_repr(self) -> str:
            return f"input_resolution={self.input_resolution}, dim={self.dim}"

        def flops(self):
            H, W = self.input_resolution
            flops = H * W * self.dim
            flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
            return flops

    class BasicLayer(nn.Module):
        """ A basic Swin Transformer layer for one stage.

        Args:
            dim (int): Number of input channels.
            input_resolution (tuple[int]): Input resolution.
            depth (int): Number of blocks.
            num_heads (int): Number of attention heads.
            window_size (int): Local window size.
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
            drop (float, optional): Dropout rate. Default: 0.0
            attn_drop (float, optional): Attention dropout rate. Default: 0.0
            drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
            norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
            downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
        """

        def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                     mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                     drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):

            super().__init__()
            self.dim = dim
            self.input_resolution = input_resolution
            self.depth = depth
            self.use_checkpoint = use_checkpoint

            # build blocks
            self.blocks = nn.ModuleList([
                SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                     num_heads=num_heads, window_size=window_size,
                                     shift_size=0 if (i % 2 == 0) else window_size // 2,
                                     mlp_ratio=mlp_ratio,
                                     qkv_bias=qkv_bias, qk_scale=qk_scale,
                                     drop=drop, attn_drop=attn_drop,
                                     drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                     norm_layer=norm_layer)
                for i in range(depth)])

            # patch merging layer
            if downsample is not None:
                self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
            else:
                self.downsample = None

        def forward(self, x, x_size):
            for blk in self.blocks:
                if self.use_checkpoint:
                    x = checkpoint.checkpoint(blk, x, x_size)
                else:
                    x = blk(x, x_size)
            if self.downsample is not None:
                x = self.downsample(x)
            return x

        def extra_repr(self) -> str:
            return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

        def flops(self):
            flops = 0
            for blk in self.blocks:
                flops += blk.flops()
            if self.downsample is not None:
                flops += self.downsample.flops()
            return flops

    class RSTB(nn.Module):
        """Residual Swin Transformer Block (RSTB).

        Args:
            dim (int): Number of input channels.
            input_resolution (tuple[int]): Input resolution.
            depth (int): Number of blocks.
            num_heads (int): Number of attention heads.
            window_size (int): Local window size.
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
            drop (float, optional): Dropout rate. Default: 0.0
            attn_drop (float, optional): Attention dropout rate. Default: 0.0
            drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
            norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
            downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
            img_size: Input image size.
            patch_size: Patch size.
            resi_connection: The convolutional block before residual connection.
        """

        def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                     mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                     drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,
                     img_size=224, patch_size=4, resi_connection='1conv'):
            super(RSTB, self).__init__()

            self.dim = dim
            self.input_resolution = input_resolution

            self.residual_group = BasicLayer(dim=dim,
                                             input_resolution=input_resolution,
                                             depth=depth,
                                             num_heads=num_heads,
                                             window_size=window_size,
                                             mlp_ratio=mlp_ratio,
                                             qkv_bias=qkv_bias, qk_scale=qk_scale,
                                             drop=drop, attn_drop=attn_drop,
                                             drop_path=drop_path,
                                             norm_layer=norm_layer,
                                             downsample=downsample,
                                             use_checkpoint=use_checkpoint)

            if resi_connection == '1conv':
                self.conv = nn.Conv2d(dim, dim, 3, 1, 1)
            elif resi_connection == '3conv':
                # to save parameters and memory
                self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),
                                          nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),
                                          nn.LeakyReLU(negative_slope=0.2, inplace=True),
                                          nn.Conv2d(dim // 4, dim, 3, 1, 1))

            self.patch_embed = PatchEmbed(
                img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,
                norm_layer=None)

            self.patch_unembed = PatchUnEmbed(
                img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,
                norm_layer=None)

        def forward(self, x, x_size):
            return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x

        def flops(self):
            flops = 0
            flops += self.residual_group.flops()
            H, W = self.input_resolution
            flops += H * W * self.dim * self.dim * 9
            flops += self.patch_embed.flops()
            flops += self.patch_unembed.flops()

            return flops

    class PatchEmbed(nn.Module):
        r""" Image to Patch Embedding

        Args:
            img_size (int): Image size.  Default: 224.
            patch_size (int): Patch token size. Default: 4.
            in_chans (int): Number of input image channels. Default: 3.
            embed_dim (int): Number of linear projection output channels. Default: 96.
            norm_layer (nn.Module, optional): Normalization layer. Default: None
        """

        def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
            super().__init__()
            img_size = to_2tuple(img_size)
            patch_size = to_2tuple(patch_size)
            patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
            self.img_size = img_size
            self.patch_size = patch_size
            self.patches_resolution = patches_resolution
            self.num_patches = patches_resolution[0] * patches_resolution[1]

            self.in_chans = in_chans
            self.embed_dim = embed_dim

            if norm_layer is not None:
                self.norm = norm_layer(embed_dim)
            else:
                self.norm = None

        def forward(self, x):
            x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C
            if self.norm is not None:
                x = self.norm(x)
            return x

        def flops(self):
            flops = 0
            H, W = self.img_size
            if self.norm is not None:
                flops += H * W * self.embed_dim
            return flops

    class PatchUnEmbed(nn.Module):
        r""" Image to Patch Unembedding

        Args:
            img_size (int): Image size.  Default: 224.
            patch_size (int): Patch token size. Default: 4.
            in_chans (int): Number of input image channels. Default: 3.
            embed_dim (int): Number of linear projection output channels. Default: 96.
            norm_layer (nn.Module, optional): Normalization layer. Default: None
        """

        def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
            super().__init__()
            img_size = to_2tuple(img_size)
            patch_size = to_2tuple(patch_size)
            patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
            self.img_size = img_size
            self.patch_size = patch_size
            self.patches_resolution = patches_resolution
            self.num_patches = patches_resolution[0] * patches_resolution[1]

            self.in_chans = in_chans
            self.embed_dim = embed_dim

        def forward(self, x, x_size):
            B, HW, C = x.shape
            x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C
            return x

        def flops(self):
            flops = 0
            return flops

    class Upsample(nn.Sequential):
        """Upsample module.

        Args:
            scale (int): Scale factor. Supported scales: 2^n and 3.
            num_feat (int): Channel number of intermediate features.
        """

        def __init__(self, scale, num_feat):
            m = []
            if (scale & (scale - 1)) == 0:  # scale = 2^n
                for _ in range(int(math.log(scale, 2))):
                    m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))
                    m.append(nn.PixelShuffle(2))
            elif scale == 3:
                m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))
                m.append(nn.PixelShuffle(3))
            else:
                raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')
            super(Upsample, self).__init__(*m)

    class UpsampleOneStep(nn.Sequential):
        """UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)
           Used in lightweight SR to save parameters.

        Args:
            scale (int): Scale factor. Supported scales: 2^n and 3.
            num_feat (int): Channel number of intermediate features.

        """

        def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):
            self.num_feat = num_feat
            self.input_resolution = input_resolution
            m = []
            m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))
            m.append(nn.PixelShuffle(scale))
            super(UpsampleOneStep, self).__init__(*m)

        def flops(self):
            H, W = self.input_resolution
            flops = H * W * self.num_feat * 3 * 9
            return flops

    class SwinIR(nn.Module):
        r""" SwinIR
            A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.

        Args:
            img_size (int | tuple(int)): Input image size. Default 64
            patch_size (int | tuple(int)): Patch size. Default: 1
            in_chans (int): Number of input image channels. Default: 3
            embed_dim (int): Patch embedding dimension. Default: 96
            depths (tuple(int)): Depth of each Swin Transformer layer.
            num_heads (tuple(int)): Number of attention heads in different layers.
            window_size (int): Window size. Default: 7
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
            qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
            qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
            drop_rate (float): Dropout rate. Default: 0
            attn_drop_rate (float): Attention dropout rate. Default: 0
            drop_path_rate (float): Stochastic depth rate. Default: 0.1
            norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
            ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
            patch_norm (bool): If True, add normalization after patch embedding. Default: True
            use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
            upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction
            img_range: Image range. 1. or 255.
            upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None
            resi_connection: The convolutional block before residual connection. '1conv'/'3conv'
        """

        def __init__(self, img_size=128, patch_size=1, in_chans=3,
                     embed_dim=180, depths=[6, 6, 6, 6, 6, 6], num_heads=[6, 6, 6, 6, 6, 6],
                     window_size=8, mlp_ratio=2., qkv_bias=True, qk_scale=None,
                     drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                     norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                     use_checkpoint=False, upscale=1, img_range=1., upsampler='', resi_connection='1conv',
                     **kwargs):
            super(SwinIR, self).__init__()
            num_in_ch = in_chans
            num_out_ch = in_chans
            num_feat = 64
            self.img_range = img_range
            if in_chans == 3:
                rgb_mean = (0.4488, 0.4371, 0.4040)
                self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)
            else:
                self.mean = torch.zeros(1, 1, 1, 1)
            self.upscale = upscale
            self.upsampler = upsampler
            self.window_size = window_size

            #####################################################################################################
            ################################### 1, shallow feature extraction ###################################
            self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)

            #####################################################################################################
            ################################### 2, deep feature extraction ######################################
            self.num_layers = len(depths)
            self.embed_dim = embed_dim
            self.ape = ape
            self.patch_norm = patch_norm
            self.num_features = embed_dim
            self.mlp_ratio = mlp_ratio

            # split image into non-overlapping patches
            self.patch_embed = PatchEmbed(
                img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,
                norm_layer=norm_layer if self.patch_norm else None)
            num_patches = self.patch_embed.num_patches
            patches_resolution = self.patch_embed.patches_resolution
            self.patches_resolution = patches_resolution

            # merge non-overlapping patches into image
            self.patch_unembed = PatchUnEmbed(
                img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,
                norm_layer=norm_layer if self.patch_norm else None)

            # absolute position embedding
            if self.ape:
                self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
                trunc_normal_(self.absolute_pos_embed, std=.02)

            self.pos_drop = nn.Dropout(p=drop_rate)

            # stochastic depth
            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

            # build Residual Swin Transformer blocks (RSTB)
            self.layers = nn.ModuleList()
            for i_layer in range(self.num_layers):
                layer = RSTB(dim=embed_dim,
                             input_resolution=(patches_resolution[0],
                                               patches_resolution[1]),
                             depth=depths[i_layer],
                             num_heads=num_heads[i_layer],
                             window_size=window_size,
                             mlp_ratio=self.mlp_ratio,
                             qkv_bias=qkv_bias, qk_scale=qk_scale,
                             drop=drop_rate, attn_drop=attn_drop_rate,
                             drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results
                             norm_layer=norm_layer,
                             downsample=None,
                             use_checkpoint=use_checkpoint,
                             img_size=img_size,
                             patch_size=patch_size,
                             resi_connection=resi_connection

                             )
                self.layers.append(layer)
            self.norm = norm_layer(self.num_features)

            # build the last conv layer in deep feature extraction
            if resi_connection == '1conv':
                self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)
            elif resi_connection == '3conv':
                # to save parameters and memory
                self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),
                                                     nn.LeakyReLU(negative_slope=0.2, inplace=True),
                                                     nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),
                                                     nn.LeakyReLU(negative_slope=0.2, inplace=True),
                                                     nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))

            #####################################################################################################
            ################################ 3, high quality image reconstruction ################################
            if self.upsampler == 'pixelshuffle':
                # for classical SR
                self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),
                                                          nn.LeakyReLU(inplace=True))
                self.upsample = Upsample(upscale, num_feat)
                self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)
            elif self.upsampler == 'pixelshuffledirect':
                # for lightweight SR (to save parameters)
                self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,
                                                (patches_resolution[0], patches_resolution[1]))
            elif self.upsampler == 'nearest+conv':
                # for real-world SR (less artifacts)
                self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),
                                                          nn.LeakyReLU(inplace=True))
                self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)
                if self.upscale == 4:
                    self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)
                self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)
                self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)
                self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)
            else:
                # for image denoising and JPEG compression artifact reduction
                self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)

            self.apply(self._init_weights)

        def _init_weights(self, m):
            if isinstance(m, nn.Linear):
                trunc_normal_(m.weight, std=.02)
                if isinstance(m, nn.Linear) and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

        @torch.jit.ignore
        def no_weight_decay(self):
            return {'absolute_pos_embed'}

        @torch.jit.ignore
        def no_weight_decay_keywords(self):
            return {'relative_position_bias_table'}

        def check_image_size(self, x):
            _, _, h, w = x.size()
            mod_pad_h = (self.window_size - h % self.window_size) % self.window_size
            mod_pad_w = (self.window_size - w % self.window_size) % self.window_size
            x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')
            return x

        def forward_features(self, x):
            x_size = (x.shape[2], x.shape[3])
            x = self.patch_embed(x)
            if self.ape:
                x = x + self.absolute_pos_embed
            x = self.pos_drop(x)

            for layer in self.layers:
                x = layer(x, x_size)

            x = self.norm(x)  # B L C
            x = self.patch_unembed(x, x_size)

            return x

        def forward(self, x):
            H, W = x.shape[2:]
            x = self.check_image_size(x)

            self.mean = self.mean.type_as(x)
            x = (x - self.mean) * self.img_range

            if self.upsampler == 'pixelshuffle':
                # for classical SR
                x = self.conv_first(x)
                x = self.conv_after_body(self.forward_features(x)) + x
                x = self.conv_before_upsample(x)
                x = self.conv_last(self.upsample(x))
            elif self.upsampler == 'pixelshuffledirect':
                # for lightweight SR
                x = self.conv_first(x)
                x = self.conv_after_body(self.forward_features(x)) + x
                x = self.upsample(x)
            elif self.upsampler == 'nearest+conv':
                # for real-world SR
                x = self.conv_first(x)
                x = self.conv_after_body(self.forward_features(x)) + x
                x = self.conv_before_upsample(x)
                x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))
                if self.upscale == 4:
                    x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))
                x = self.conv_last(self.lrelu(self.conv_hr(x)))
            else:
                # for image denoising and JPEG compression artifact reduction
                x_first = self.conv_first(x)
                res = self.conv_after_body(self.forward_features(x_first)) + x_first
                x = x + self.conv_last(res)

            x = x / self.img_range + self.mean

            return x[:, :, :H * self.upscale, :W * self.upscale]

        def flops(self):
            flops = 0
            H, W = self.patches_resolution
            flops += H * W * 3 * self.embed_dim * 9
            flops += self.patch_embed.flops()
            for i, layer in enumerate(self.layers):
                flops += layer.flops()
            flops += H * W * 3 * self.embed_dim * self.embed_dim
            flops += self.upsample.flops()
            return flops

    return SwinIR


def get_restormer():
    ##############################################################
    ################            Restormer             ############
    ##############################################################
    ## Restormer: Efficient Transformer for High-Resolution Image Restoration
    ## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang
    ## https://arxiv.org/abs/2111.09881

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import numbers

    ##########################################################################
    ## Layer Norm

    def to_3d(x):
        return rearrange(x, 'b c h w -> b (h w) c')

    def to_4d(x, h, w):
        return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)

    class BiasFree_LayerNorm(nn.Module):
        def __init__(self, normalized_shape):
            super(BiasFree_LayerNorm, self).__init__()
            if isinstance(normalized_shape, numbers.Integral):
                normalized_shape = (normalized_shape,)
            normalized_shape = torch.Size(normalized_shape)

            assert len(normalized_shape) == 1

            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.normalized_shape = normalized_shape

        def forward(self, x):
            sigma = x.var(-1, keepdim=True, unbiased=False)
            return x / torch.sqrt(sigma + 1e-5) * self.weight

    class WithBias_LayerNorm(nn.Module):
        def __init__(self, normalized_shape):
            super(WithBias_LayerNorm, self).__init__()
            if isinstance(normalized_shape, numbers.Integral):
                normalized_shape = (normalized_shape,)
            normalized_shape = torch.Size(normalized_shape)

            assert len(normalized_shape) == 1

            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.bias = nn.Parameter(torch.zeros(normalized_shape))
            self.normalized_shape = normalized_shape

        def forward(self, x):
            mu = x.mean(-1, keepdim=True)
            sigma = x.var(-1, keepdim=True, unbiased=False)
            return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias

    class LayerNorm(nn.Module):
        def __init__(self, dim, LayerNorm_type):
            super(LayerNorm, self).__init__()
            if LayerNorm_type == 'BiasFree':
                self.body = BiasFree_LayerNorm(dim)
            else:
                self.body = WithBias_LayerNorm(dim)

        def forward(self, x):
            h, w = x.shape[-2:]
            return to_4d(self.body(to_3d(x)), h, w)

    ##########################################################################
    ## Gated-Dconv Feed-Forward Network (GDFN)
    class FeedForward(nn.Module):
        def __init__(self, dim, ffn_expansion_factor, bias):
            super(FeedForward, self).__init__()

            hidden_features = int(dim * ffn_expansion_factor)

            self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)

            self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1, groups=hidden_features * 2, bias=bias)

            self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)

        def forward(self, x):
            x = self.project_in(x)
            x1, x2 = self.dwconv(x).chunk(2, dim=1)
            x = F.gelu(x1) * x2
            x = self.project_out(x)
            return x

    ##########################################################################
    ## Multi-DConv Head Transposed Self-Attention (MDTA)
    class Attention(nn.Module):
        def __init__(self, dim, num_heads, bias):
            super(Attention, self).__init__()
            self.num_heads = num_heads
            self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))

            self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)
            self.qkv_dwconv = nn.Conv2d(dim * 3, dim * 3, kernel_size=3, stride=1, padding=1, groups=dim * 3, bias=bias)
            self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

        def forward(self, x):
            b, c, h, w = x.shape

            qkv = self.qkv_dwconv(self.qkv(x))
            q, k, v = qkv.chunk(3, dim=1)

            q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
            k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
            v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)

            q = torch.nn.functional.normalize(q, dim=-1)
            k = torch.nn.functional.normalize(k, dim=-1)

            attn = (q @ k.transpose(-2, -1)) * self.temperature
            attn = attn.softmax(dim=-1)

            out = (attn @ v)

            out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)

            out = self.project_out(out)
            return out

    ##########################################################################
    class TransformerBlock(nn.Module):
        def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):
            super(TransformerBlock, self).__init__()

            self.norm1 = LayerNorm(dim, LayerNorm_type)
            self.attn = Attention(dim, num_heads, bias)
            self.norm2 = LayerNorm(dim, LayerNorm_type)
            self.ffn = FeedForward(dim, ffn_expansion_factor, bias)

        def forward(self, x):
            x = x + self.attn(self.norm1(x))
            x = x + self.ffn(self.norm2(x))

            return x

    ##########################################################################
    ## Overlapped image patch embedding with 3x3 Conv
    class OverlapPatchEmbed(nn.Module):
        def __init__(self, in_c=3, embed_dim=48, bias=False):
            super(OverlapPatchEmbed, self).__init__()

            self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)

        def forward(self, x):
            x = self.proj(x)

            return x

    ##########################################################################
    ## Resizing modules
    class Downsample(nn.Module):
        def __init__(self, n_feat):
            super(Downsample, self).__init__()

            self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat // 2, kernel_size=3, stride=1, padding=1, bias=False),
                                      nn.PixelUnshuffle(2))

        def forward(self, x):
            return self.body(x)

    class Upsample(nn.Module):
        def __init__(self, n_feat):
            super(Upsample, self).__init__()

            self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat * 2, kernel_size=3, stride=1, padding=1, bias=False),
                                      nn.PixelShuffle(2))

        def forward(self, x):
            return self.body(x)

    ##########################################################################
    ##---------- Restormer -----------------------
    class Restormer(nn.Module):
        def __init__(self,
                     inp_channels=3,
                     out_channels=3,
                     dim=48,
                     num_blocks=[4, 6, 6, 8],
                     num_refinement_blocks=4,
                     heads=[1, 2, 4, 8],
                     ffn_expansion_factor=2.66,
                     bias=False,
                     LayerNorm_type='BiasFree',  ## Other option 'BiasFree'
                     dual_pixel_task=False  ## True for dual-pixel defocus deblurring only. Also set inp_channels=6
                     ):

            super(Restormer, self).__init__()

            self.patch_embed = OverlapPatchEmbed(inp_channels, dim)

            self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])

            self.down1_2 = Downsample(dim)  ## From Level 1 to Level 2
            self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])

            self.down2_3 = Downsample(int(dim * 2 ** 1))  ## From Level 2 to Level 3
            self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])

            self.down3_4 = Downsample(int(dim * 2 ** 2))  ## From Level 3 to Level 4
            self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim * 2 ** 3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])

            self.up4_3 = Upsample(int(dim * 2 ** 3))  ## From Level 4 to Level 3
            self.reduce_chan_level3 = nn.Conv2d(int(dim * 2 ** 3), int(dim * 2 ** 2), kernel_size=1, bias=bias)
            self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])

            self.up3_2 = Upsample(int(dim * 2 ** 2))  ## From Level 3 to Level 2
            self.reduce_chan_level2 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)
            self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])

            self.up2_1 = Upsample(int(dim * 2 ** 1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)

            self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])

            self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])

            #### For Dual-Pixel Defocus Deblurring Task ####
            self.dual_pixel_task = dual_pixel_task
            if self.dual_pixel_task:
                self.skip_conv = nn.Conv2d(dim, int(dim * 2 ** 1), kernel_size=1, bias=bias)
            ###########################

            self.output = nn.Conv2d(int(dim * 2 ** 1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)

        def forward(self, inp_img):

            inp_enc_level1 = self.patch_embed(inp_img)
            out_enc_level1 = self.encoder_level1(inp_enc_level1)

            inp_enc_level2 = self.down1_2(out_enc_level1)
            out_enc_level2 = self.encoder_level2(inp_enc_level2)

            inp_enc_level3 = self.down2_3(out_enc_level2)
            out_enc_level3 = self.encoder_level3(inp_enc_level3)

            inp_enc_level4 = self.down3_4(out_enc_level3)
            latent = self.latent(inp_enc_level4)

            inp_dec_level3 = self.up4_3(latent)
            inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)
            inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)
            out_dec_level3 = self.decoder_level3(inp_dec_level3)

            inp_dec_level2 = self.up3_2(out_dec_level3)
            inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)
            inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)
            out_dec_level2 = self.decoder_level2(inp_dec_level2)

            inp_dec_level1 = self.up2_1(out_dec_level2)
            inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)
            out_dec_level1 = self.decoder_level1(inp_dec_level1)

            out_dec_level1 = self.refinement(out_dec_level1)

            #### For Dual-Pixel Defocus Deblurring Task ####
            if self.dual_pixel_task:
                out_dec_level1 = out_dec_level1 + self.skip_conv(inp_enc_level1)
                out_dec_level1 = self.output(out_dec_level1)
            ###########################
            else:
                out_dec_level1 = self.output(out_dec_level1) + inp_img

            return out_dec_level1

    return Restormer


def get_rrdb():
    ##############################################################
    ################            RRDB NET              ############
    ##############################################################
    import torch
    from torch import nn as nn

    def make_layer(basic_block, num_basic_block, **kwarg):
        """Make layers by stacking the same blocks.

        Args:
            basic_block (nn.module): nn.module class for basic block.
            num_basic_block (int): number of blocks.

        Returns:
            nn.Sequential: Stacked blocks in nn.Sequential.
        """
        layers = []
        for _ in range(num_basic_block):
            layers.append(basic_block(**kwarg))
        return nn.Sequential(*layers)

    class ResidualDenseBlock(nn.Module):
        """Residual Dense Block.

        Used in RRDB block in ESRGAN.

        Args:
            num_feat (int): Channel number of intermediate features.
            num_grow_ch (int): Channels for each growth.
        """

        def __init__(self, num_feat=64, num_grow_ch=32):
            super(ResidualDenseBlock, self).__init__()
            self.conv1 = nn.Conv2d(num_feat, num_grow_ch, 3, 1, 1)
            self.conv2 = nn.Conv2d(num_feat + num_grow_ch, num_grow_ch, 3, 1, 1)
            self.conv3 = nn.Conv2d(num_feat + 2 * num_grow_ch, num_grow_ch, 3, 1, 1)
            self.conv4 = nn.Conv2d(num_feat + 3 * num_grow_ch, num_grow_ch, 3, 1, 1)
            self.conv5 = nn.Conv2d(num_feat + 4 * num_grow_ch, num_feat, 3, 1, 1)

            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)

        def forward(self, x):
            x1 = self.lrelu(self.conv1(x))
            x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))
            x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))
            x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
            x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))
            # Empirically, we use 0.2 to scale the residual for better performance
            return x5 * 0.2 + x

    class RRDB(nn.Module):
        """Residual in Residual Dense Block.

        Used in RRDB-Net in ESRGAN.

        Args:
            num_feat (int): Channel number of intermediate features.
            num_grow_ch (int): Channels for each growth.
        """

        def __init__(self, num_feat, num_grow_ch=32):
            super(RRDB, self).__init__()
            self.rdb1 = ResidualDenseBlock(num_feat, num_grow_ch)
            self.rdb2 = ResidualDenseBlock(num_feat, num_grow_ch)
            self.rdb3 = ResidualDenseBlock(num_feat, num_grow_ch)

        def forward(self, x):
            out = self.rdb1(x)
            out = self.rdb2(out)
            out = self.rdb3(out)
            # Empirically, we use 0.2 to scale the residual for better performance
            return out * 0.2 + x

    class RRDBNet(nn.Module):
        """Networks consisting of Residual in Residual Dense Block, which is used
        in ESRGAN.

        ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks.

        We extend ESRGAN for scale x2 and scale x1.
        Note: This is one option for scale 1, scale 2 in RRDBNet.
        We first employ the pixel-unshuffle (an inverse operation of pixelshuffle to reduce the spatial size
        and enlarge the channel size before feeding inputs into the main ESRGAN architecture.

        Args:
            num_in_ch (int): Channel number of inputs.
            num_out_ch (int): Channel number of outputs.
            num_feat (int): Channel number of intermediate features.
                Default: 64
            num_block (int): Block number in the trunk network. Defaults: 23
            num_grow_ch (int): Channels for each growth. Default: 32.
        """

        def __init__(self, num_in_ch=18, num_out_ch=3, num_feat=64, num_block=16, num_grow_ch=32):
            super(RRDBNet, self).__init__()

            self.conv_first = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1)

            self.body = make_layer(RRDB, num_block, num_feat=num_feat, num_grow_ch=num_grow_ch)
            self.conv_body = nn.Conv2d(num_feat, num_feat, 3, 1, 1)

            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)

        def forward(self, x):
            feat = self.conv_first(x)

            feat = self.conv_body(self.body(feat)) + feat

            feat = self.conv_last(feat)
            return feat + x[:, :3, :, :]

    return RRDBNet


def get_xrestormer():
    import math
    import numbers

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from einops import rearrange
    from torch import einsum

    def to(x):
        return {'device': x.device, 'dtype': x.dtype}

    def pair(x):
        return (x, x) if not isinstance(x, tuple) else x

    def expand_dim(t, dim, k):
        t = t.unsqueeze(dim=dim)
        expand_shape = [-1] * len(t.shape)
        expand_shape[dim] = k
        return t.expand(*expand_shape)

    def rel_to_abs(x):
        b, l, m = x.shape
        r = (m + 1) // 2

        col_pad = torch.zeros((b, l, 1), **to(x))
        x = torch.cat((x, col_pad), dim=2)
        flat_x = rearrange(x, 'b l c -> b (l c)')
        flat_pad = torch.zeros((b, m - l), **to(x))
        flat_x_padded = torch.cat((flat_x, flat_pad), dim=1)
        final_x = flat_x_padded.reshape(b, l + 1, m)
        final_x = final_x[:, :l, -r:]
        return final_x

    def relative_logits_1d(q, rel_k):
        b, h, w, _ = q.shape
        r = (rel_k.shape[0] + 1) // 2

        logits = einsum('b x y d, r d -> b x y r', q, rel_k)
        logits = rearrange(logits, 'b x y r -> (b x) y r')
        logits = rel_to_abs(logits)

        logits = logits.reshape(b, h, w, r)
        logits = expand_dim(logits, dim=2, k=r)
        return logits

    class RelPosEmb(nn.Module):
        def __init__(
                self,
                block_size,
                rel_size,
                dim_head
        ):
            super().__init__()
            height = width = rel_size
            scale = dim_head ** -0.5

            self.block_size = block_size
            self.rel_height = nn.Parameter(torch.randn(height * 2 - 1, dim_head) * scale)
            self.rel_width = nn.Parameter(torch.randn(width * 2 - 1, dim_head) * scale)

        def forward(self, q):
            block = self.block_size

            q = rearrange(q, 'b (x y) c -> b x y c', x=block)
            rel_logits_w = relative_logits_1d(q, self.rel_width)
            rel_logits_w = rearrange(rel_logits_w, 'b x i y j-> b (x y) (i j)')

            q = rearrange(q, 'b x y d -> b y x d')
            rel_logits_h = relative_logits_1d(q, self.rel_height)
            rel_logits_h = rearrange(rel_logits_h, 'b x i y j -> b (y x) (j i)')
            return rel_logits_w + rel_logits_h

    ##########################################################################
    ## Layer Norm

    def to_3d(x):
        return rearrange(x, 'b c h w -> b (h w) c')

    def to_4d(x, h, w):
        return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)

    class BiasFree_LayerNorm(nn.Module):
        def __init__(self, normalized_shape):
            super(BiasFree_LayerNorm, self).__init__()
            if isinstance(normalized_shape, numbers.Integral):
                normalized_shape = (normalized_shape,)
            normalized_shape = torch.Size(normalized_shape)

            assert len(normalized_shape) == 1

            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.normalized_shape = normalized_shape

        def forward(self, x):
            sigma = x.var(-1, keepdim=True, unbiased=False)
            return x / torch.sqrt(sigma + 1e-5) * self.weight

    class WithBias_LayerNorm(nn.Module):
        def __init__(self, normalized_shape):
            super(WithBias_LayerNorm, self).__init__()
            if isinstance(normalized_shape, numbers.Integral):
                normalized_shape = (normalized_shape,)
            normalized_shape = torch.Size(normalized_shape)

            assert len(normalized_shape) == 1

            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.bias = nn.Parameter(torch.zeros(normalized_shape))
            self.normalized_shape = normalized_shape

        def forward(self, x):
            mu = x.mean(-1, keepdim=True)
            sigma = x.var(-1, keepdim=True, unbiased=False)
            return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias

    class LayerNorm(nn.Module):
        def __init__(self, dim, LayerNorm_type):
            super(LayerNorm, self).__init__()
            if LayerNorm_type == 'BiasFree':
                self.body = BiasFree_LayerNorm(dim)
            else:
                self.body = WithBias_LayerNorm(dim)

        def forward(self, x):
            h, w = x.shape[-2:]
            return to_4d(self.body(to_3d(x)), h, w)

    ##########################################################################
    ## Gated-Dconv Feed-Forward Network (GDFN)
    class FeedForward(nn.Module):
        def __init__(self, dim, ffn_expansion_factor, bias):
            super(FeedForward, self).__init__()

            hidden_features = int(dim * ffn_expansion_factor)

            self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)

            self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1, groups=hidden_features * 2, bias=bias)

            self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)

        def forward(self, x):
            x = self.project_in(x)
            x1, x2 = self.dwconv(x).chunk(2, dim=1)
            x = F.gelu(x1) * x2
            x = self.project_out(x)
            return x

    ##########################################################################
    ## Multi-DConv Head Transposed Self-Attention (MDTA)
    class ChannelAttention(nn.Module):
        def __init__(self, dim, num_heads, bias):
            super(ChannelAttention, self).__init__()
            self.num_heads = num_heads
            self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))

            self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)
            self.qkv_dwconv = nn.Conv2d(dim * 3, dim * 3, kernel_size=3, stride=1, padding=1, groups=dim * 3, bias=bias)
            self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

        def forward(self, x):
            b, c, h, w = x.shape

            qkv = self.qkv_dwconv(self.qkv(x))
            q, k, v = qkv.chunk(3, dim=1)

            q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
            k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
            v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)

            q = torch.nn.functional.normalize(q, dim=-1)
            k = torch.nn.functional.normalize(k, dim=-1)

            attn = (q @ k.transpose(-2, -1)) * self.temperature
            attn = attn.softmax(dim=-1)

            out = (attn @ v)

            out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)

            out = self.project_out(out)
            return out

    ##########################################################################
    ## Overlapping Cross-Attention (OCA)
    class OCAB(nn.Module):
        def __init__(self, dim, window_size, overlap_ratio, num_heads, dim_head, bias):
            super(OCAB, self).__init__()
            self.num_spatial_heads = num_heads
            self.dim = dim
            self.window_size = window_size
            self.overlap_win_size = int(window_size * overlap_ratio) + window_size
            self.dim_head = dim_head
            self.inner_dim = self.dim_head * self.num_spatial_heads
            self.scale = self.dim_head ** -0.5

            self.unfold = nn.Unfold(kernel_size=(self.overlap_win_size, self.overlap_win_size), stride=window_size, padding=(self.overlap_win_size - window_size) // 2)
            self.qkv = nn.Conv2d(self.dim, self.inner_dim * 3, kernel_size=1, bias=bias)
            self.project_out = nn.Conv2d(self.inner_dim, dim, kernel_size=1, bias=bias)
            self.rel_pos_emb = RelPosEmb(
                block_size=window_size,
                rel_size=window_size + (self.overlap_win_size - window_size),
                dim_head=self.dim_head
            )

        def forward(self, x):
            b, c, h, w = x.shape
            qkv = self.qkv(x)
            qs, ks, vs = qkv.chunk(3, dim=1)

            # spatial attention
            qs = rearrange(qs, 'b c (h p1) (w p2) -> (b h w) (p1 p2) c', p1=self.window_size, p2=self.window_size)
            ks, vs = map(lambda t: self.unfold(t), (ks, vs))
            ks, vs = map(lambda t: rearrange(t, 'b (c j) i -> (b i) j c', c=self.inner_dim), (ks, vs))

            # print(f'qs.shape:{qs.shape}, ks.shape:{ks.shape}, vs.shape:{vs.shape}')
            # split heads
            qs, ks, vs = map(lambda t: rearrange(t, 'b n (head c) -> (b head) n c', head=self.num_spatial_heads), (qs, ks, vs))

            # attention
            qs = qs * self.scale
            spatial_attn = (qs @ ks.transpose(-2, -1))
            spatial_attn += self.rel_pos_emb(qs)
            spatial_attn = spatial_attn.softmax(dim=-1)

            out = (spatial_attn @ vs)

            out = rearrange(out, '(b h w head) (p1 p2) c -> b (head c) (h p1) (w p2)', head=self.num_spatial_heads, h=h // self.window_size, w=w // self.window_size, p1=self.window_size, p2=self.window_size)

            # merge spatial and channel
            out = self.project_out(out)

            return out

    ##########################################################################
    class TransformerBlock(nn.Module):
        def __init__(self, dim, window_size, overlap_ratio, num_channel_heads, num_spatial_heads, spatial_dim_head, ffn_expansion_factor, bias, LayerNorm_type):
            super(TransformerBlock, self).__init__()

            self.spatial_attn = OCAB(dim, window_size, overlap_ratio, num_spatial_heads, spatial_dim_head, bias)
            self.channel_attn = ChannelAttention(dim, num_channel_heads, bias)

            self.norm1 = LayerNorm(dim, LayerNorm_type)
            self.norm2 = LayerNorm(dim, LayerNorm_type)
            self.norm3 = LayerNorm(dim, LayerNorm_type)
            self.norm4 = LayerNorm(dim, LayerNorm_type)

            self.channel_ffn = FeedForward(dim, ffn_expansion_factor, bias)
            self.spatial_ffn = FeedForward(dim, ffn_expansion_factor, bias)

        def forward(self, x):
            x = x + self.channel_attn(self.norm1(x))
            x = x + self.channel_ffn(self.norm2(x))
            x = x + self.spatial_attn(self.norm3(x))
            x = x + self.spatial_ffn(self.norm4(x))
            return x

    ##########################################################################
    ## Overlapped image patch embedding with 3x3 Conv
    class OverlapPatchEmbed(nn.Module):
        def __init__(self, in_c=3, embed_dim=48, bias=False):
            super(OverlapPatchEmbed, self).__init__()

            self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)

        def forward(self, x):
            x = self.proj(x)

            return x

    ##########################################################################
    ## Resizing modules
    class Downsample(nn.Module):
        def __init__(self, n_feat):
            super(Downsample, self).__init__()

            self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat // 2, kernel_size=3, stride=1, padding=1, bias=False),
                                      nn.PixelUnshuffle(2))

        def forward(self, x):
            return self.body(x)

    class Upsample(nn.Module):
        def __init__(self, n_feat):
            super(Upsample, self).__init__()

            self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat * 2, kernel_size=3, stride=1, padding=1, bias=False),
                                      nn.PixelShuffle(2))

        def forward(self, x):
            return self.body(x)

    class SR_Upsample(nn.Sequential):
        """SR_Upsample module.
        Args:
            scale (int): Scale factor. Supported scales: 2^n and 3.
            num_feat (int): Channel number of features.
        """

        def __init__(self, scale, num_feat):
            m = []

            if (scale & (scale - 1)) == 0:  # scale = 2^n
                for _ in range(int(math.log(scale, 2))):
                    m.append(nn.Conv2d(num_feat, 4 * num_feat, kernel_size=3, stride=1, padding=1))
                    m.append(nn.PixelShuffle(2))
            elif scale == 3:
                m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))
                m.append(nn.PixelShuffle(3))
            else:
                raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')
            super(SR_Upsample, self).__init__(*m)

    ##########################################################################

    class XRestormer(nn.Module):
        def __init__(self,
                     inp_channels=3,
                     out_channels=3,
                     dim=48,
                     num_blocks=(2, 4, 4, 4),
                     num_refinement_blocks=4,
                     channel_heads=(1, 2, 4, 8),
                     spatial_heads=(1, 2, 4, 8),
                     overlap_ratio=(0.5, 0.5, 0.5, 0.5),
                     window_size=8,
                     spatial_dim_head=16,
                     bias=False,
                     ffn_expansion_factor=2.66,
                     LayerNorm_type='WithBias',  ## Other option 'BiasFree'
                     dual_pixel_task=False,  ## True for dual-pixel defocus deblurring only. Also set inp_channels=6
                     scale=1
                     ):
            super(XRestormer, self).__init__()
            print("Initializing XRestormer")
            self.scale = scale

            self.patch_embed = OverlapPatchEmbed(inp_channels, dim)
            self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, window_size=window_size, overlap_ratio=overlap_ratio[0], num_channel_heads=channel_heads[0], num_spatial_heads=spatial_heads[0], spatial_dim_head=spatial_dim_head,
                                                                   ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])

            self.down1_2 = Downsample(dim)  ## From Level 1 to Level 2
            self.encoder_level2 = nn.Sequential(*[
                TransformerBlock(dim=int(dim * 2 ** 1), window_size=window_size, overlap_ratio=overlap_ratio[1], num_channel_heads=channel_heads[1], num_spatial_heads=spatial_heads[1], spatial_dim_head=spatial_dim_head,
                                 ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])

            self.down2_3 = Downsample(int(dim * 2 ** 1))  ## From Level 2 to Level 3
            self.encoder_level3 = nn.Sequential(*[
                TransformerBlock(dim=int(dim * 2 ** 2), window_size=window_size, overlap_ratio=overlap_ratio[2], num_channel_heads=channel_heads[2], num_spatial_heads=spatial_heads[2], spatial_dim_head=spatial_dim_head,
                                 ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])

            self.down3_4 = Downsample(int(dim * 2 ** 2))  ## From Level 3 to Level 4
            self.latent = nn.Sequential(*[
                TransformerBlock(dim=int(dim * 2 ** 3), window_size=window_size, overlap_ratio=overlap_ratio[3], num_channel_heads=channel_heads[3], num_spatial_heads=spatial_heads[3], spatial_dim_head=spatial_dim_head,
                                 ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])

            self.up4_3 = Upsample(int(dim * 2 ** 3))  ## From Level 4 to Level 3
            self.reduce_chan_level3 = nn.Conv2d(int(dim * 2 ** 3), int(dim * 2 ** 2), kernel_size=1, bias=bias)
            self.decoder_level3 = nn.Sequential(*[
                TransformerBlock(dim=int(dim * 2 ** 2), window_size=window_size, overlap_ratio=overlap_ratio[2], num_channel_heads=channel_heads[2], num_spatial_heads=spatial_heads[2], spatial_dim_head=spatial_dim_head,
                                 ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])

            self.up3_2 = Upsample(int(dim * 2 ** 2))  ## From Level 3 to Level 2
            self.reduce_chan_level2 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)
            self.decoder_level2 = nn.Sequential(*[
                TransformerBlock(dim=int(dim * 2 ** 1), window_size=window_size, overlap_ratio=overlap_ratio[1], num_channel_heads=channel_heads[1], num_spatial_heads=spatial_heads[1], spatial_dim_head=spatial_dim_head,
                                 ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])

            self.up2_1 = Upsample(int(dim * 2 ** 1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)

            self.decoder_level1 = nn.Sequential(*[
                TransformerBlock(dim=int(dim * 2 ** 1), window_size=window_size, overlap_ratio=overlap_ratio[0], num_channel_heads=channel_heads[0], num_spatial_heads=spatial_heads[0], spatial_dim_head=spatial_dim_head,
                                 ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])

            self.refinement = nn.Sequential(*[
                TransformerBlock(dim=int(dim * 2 ** 1), window_size=window_size, overlap_ratio=overlap_ratio[0], num_channel_heads=channel_heads[0], num_spatial_heads=spatial_heads[0], spatial_dim_head=spatial_dim_head,
                                 ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])

            self.output = nn.Conv2d(int(dim * 2 ** 1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)

        def forward(self, inp_img):
            if self.scale > 1:
                inp_img = F.interpolate(inp_img, scale_factor=self.scale, mode='bilinear', align_corners=False)

            inp_enc_level1 = self.patch_embed(inp_img)
            out_enc_level1 = self.encoder_level1(inp_enc_level1)

            inp_enc_level2 = self.down1_2(out_enc_level1)
            out_enc_level2 = self.encoder_level2(inp_enc_level2)

            inp_enc_level3 = self.down2_3(out_enc_level2)
            out_enc_level3 = self.encoder_level3(inp_enc_level3)

            inp_enc_level4 = self.down3_4(out_enc_level3)
            latent = self.latent(inp_enc_level4)

            inp_dec_level3 = self.up4_3(latent)
            inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)
            inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)
            out_dec_level3 = self.decoder_level3(inp_dec_level3)

            inp_dec_level2 = self.up3_2(out_dec_level3)
            inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)
            inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)
            out_dec_level2 = self.decoder_level2(inp_dec_level2)

            inp_dec_level1 = self.up2_1(out_dec_level2)
            inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)
            out_dec_level1 = self.decoder_level1(inp_dec_level1)

            out_dec_level1 = self.refinement(out_dec_level1)
            out_dec_level1 = self.output(out_dec_level1) + inp_img

            return out_dec_level1

    return XRestormer


def get_restormer_ex():
    # Restormer: Efficient Transformer for High-Resolution Image Restoration
    # Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang
    # https://arxiv.org/abs/2111.09881

    import numbers

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from einops import rearrange

    ##########################################################################
    ## Layer Norm

    def to_3d(x):
        return rearrange(x, 'b c h w -> b (h w) c')

    def to_4d(x, h, w):
        return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)

    def im2col(x, kernel_size, stride, padding, dilation=1):
        return F.unfold(x, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)

    class BiasFree_LayerNorm(nn.Module):
        def __init__(self, normalized_shape):
            super(BiasFree_LayerNorm, self).__init__()
            if isinstance(normalized_shape, numbers.Integral):
                normalized_shape = (normalized_shape,)
            normalized_shape = torch.Size(normalized_shape)

            assert len(normalized_shape) == 1

            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.normalized_shape = normalized_shape

        def forward(self, x):
            sigma = x.var(-1, keepdim=True, unbiased=False)
            return x / torch.sqrt(sigma + 1e-5) * self.weight

    class WithBias_LayerNorm(nn.Module):
        def __init__(self, normalized_shape):
            super(WithBias_LayerNorm, self).__init__()
            if isinstance(normalized_shape, numbers.Integral):
                normalized_shape = (normalized_shape,)
            normalized_shape = torch.Size(normalized_shape)

            assert len(normalized_shape) == 1

            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.bias = nn.Parameter(torch.zeros(normalized_shape))
            self.normalized_shape = normalized_shape

        def forward(self, x):
            mu = x.mean(-1, keepdim=True)
            sigma = x.var(-1, keepdim=True, unbiased=False)
            return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias

    class LayerNorm(nn.Module):
        def __init__(self, dim, LayerNorm_type):
            super(LayerNorm, self).__init__()
            if LayerNorm_type == 'BiasFree':
                self.body = BiasFree_LayerNorm(dim)
            else:
                self.body = WithBias_LayerNorm(dim)

        def forward(self, x):
            h, w = x.shape[-2:]
            return to_4d(self.body(to_3d(x)), h, w)

    ##########################################################################
    ## Gated-Dconv Feed-Forward Network (GDFN)
    class FeedForward(nn.Module):
        def __init__(self, dim, ffn_expansion_factor, bias):
            super(FeedForward, self).__init__()

            hidden_features = int(dim * ffn_expansion_factor)

            self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)

            self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1, groups=hidden_features * 2, bias=bias)

            self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)

        def forward(self, x):
            x = self.project_in(x)
            x1, x2 = self.dwconv(x).chunk(2, dim=1)
            x = F.gelu(x1) * x2
            x = self.project_out(x)
            return x

    ##########################################################################
    ## Multi-DConv Head Transposed Self-Attention (MDTA)
    class Attention(nn.Module):
        def __init__(self, dim, num_heads, bias):
            super(Attention, self).__init__()
            self.num_heads = num_heads
            self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))

            self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=bias)
            self.qkv_dwconv = nn.Conv2d(dim * 3, dim * 3, kernel_size=3, stride=1, padding=1, groups=dim * 3, bias=bias)
            self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

        def forward(self, x):
            b, c, h, w = x.shape

            qkv = self.qkv_dwconv(self.qkv(x))
            q, k, v = qkv.chunk(3, dim=1)

            q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
            k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
            v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)

            q = torch.nn.functional.normalize(q, dim=-1)
            k = torch.nn.functional.normalize(k, dim=-1)

            attn = (q @ k.transpose(-2, -1)) * self.temperature
            attn = attn.softmax(dim=-1)

            out = (attn @ v)

            out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)

            out = self.project_out(out)
            return out

    ##########################################################################
    ## Multi-DConv Head Transposed Self-Attention (MDTA)
    class AttentionSpatial(nn.Module):
        def __init__(self, dim, num_heads, bias, ksize):
            super(AttentionSpatial, self).__init__()
            self.ksize = ksize
            self.num_heads = max(ksize ** 2 // 128, 1)
            self.temperature = nn.Parameter(torch.ones(self.num_heads, 1, 1))

            self.qkv = nn.Conv2d(ksize ** 2, ksize ** 2 * 3, kernel_size=1, bias=bias)
            self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

        def forward(self, x):
            b, c, h, w = x.shape
            assert h % self.ksize == 0
            assert w % self.ksize == 0
            rh = h // self.ksize
            rw = w // self.ksize
            x = rearrange(x, 'b c h w -> (b c) 1 h w')
            x = F.unfold(x, self.ksize, stride=self.ksize).unsqueeze(-1)  # B K*K rh*rw 1

            q, k, v = self.qkv(x).chunk(3, dim=1)
            q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
            k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
            v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)

            q = torch.nn.functional.normalize(q, dim=-1)
            k = torch.nn.functional.normalize(k, dim=-1)

            attn = (q @ k.transpose(-2, -1)) * self.temperature
            attn = attn.softmax(dim=-1)

            out = (attn @ v)

            out = rearrange(out, 'b head c n -> b (head c) n', head=self.num_heads)
            out = F.fold(out, (h, w), self.ksize, stride=self.ksize)
            out = rearrange(out.squeeze(1), '(b c) h w -> b c h w', c=c)

            out = self.project_out(out)
            return out

    ##########################################################################
    class TransformerBlock(nn.Module):
        def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type, img_size):
            super(TransformerBlock, self).__init__()
            # channel attention
            self.norm11 = LayerNorm(dim, LayerNorm_type)
            self.attn11 = Attention(dim, num_heads, bias)
            self.norm12 = LayerNorm(dim, LayerNorm_type)
            self.ffn_11 = FeedForward(dim, ffn_expansion_factor, bias)
            # spatial attention
            self.norm21 = LayerNorm(dim, LayerNorm_type)
            self.attn21 = AttentionSpatial(dim, num_heads, bias, img_size)
            self.norm22 = LayerNorm(dim, LayerNorm_type)
            self.ffn_21 = FeedForward(dim, ffn_expansion_factor, bias)

        def forward(self, x):
            x = x + self.attn11(self.norm11(x))
            x = x + self.ffn_11(self.norm12(x))

            x = x + self.attn21(self.norm21(x))
            x = x + self.ffn_21(self.norm22(x))

            return x

    ##########################################################################
    ## Overlapped image patch embedding with 3x3 Conv
    class OverlapPatchEmbed(nn.Module):
        def __init__(self, in_c=3, embed_dim=48, bias=False):
            super(OverlapPatchEmbed, self).__init__()

            self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)

        def forward(self, x):
            x = self.proj(x)

            return x

    ##########################################################################
    ## Resizing modules
    class Downsample(nn.Module):
        def __init__(self, n_feat):
            super(Downsample, self).__init__()

            self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat // 2, kernel_size=3, stride=1, padding=1, bias=False),
                                      nn.PixelUnshuffle(2))

        def forward(self, x):
            return self.body(x)

    class Upsample(nn.Module):
        def __init__(self, n_feat):
            super(Upsample, self).__init__()

            self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat * 2, kernel_size=3, stride=1, padding=1, bias=False),
                                      nn.PixelShuffle(2))

        def forward(self, x):
            return self.body(x)

    ##########################################################################
    # ---------- Restormer -----------------------
    class RestormerEx(nn.Module):
        def __init__(self,
                     c_in=3,
                     c_out=3,
                     c_mid=48,
                     num_blocks=None,
                     num_refinement_blocks=4,
                     heads=None,
                     ffn_expansion_factor=2.66,
                     bias=False,
                     LayerNorm_type='BiasFree',  # Other option 'BiasFree'
                     dual_pixel_task=False,
                     global_sc=True,  # True for dual-pixel defocus deblurring only. Also set inp_channels=6
                     ):

            super(RestormerEx, self).__init__()

            self.global_sc = global_sc

            inp_channels = c_in
            out_channels = c_out
            dim = c_mid

            img_size = [32, 32, 16, 8]
            if heads is None:
                heads = [1, 2, 4, 8]
            if num_blocks is None:
                num_blocks = [4, 6, 6, 8]
            self.patch_embed = OverlapPatchEmbed(inp_channels, dim)

            self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type, img_size=img_size[0]) for i in range(num_blocks[0])])

            self.down1_2 = Downsample(dim)  ## From Level 1 to Level 2
            self.encoder_level2 = nn.Sequential(
                *[TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type, img_size=img_size[1]) for i in range(num_blocks[1])])

            self.down2_3 = Downsample(int(dim * 2 ** 1))  ## From Level 2 to Level 3
            self.encoder_level3 = nn.Sequential(
                *[TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type, img_size=img_size[2]) for i in range(num_blocks[2])])

            self.down3_4 = Downsample(int(dim * 2 ** 2))  ## From Level 3 to Level 4
            self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim * 2 ** 3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type, img_size=img_size[3]) for i in range(num_blocks[3])])

            self.up4_3 = Upsample(int(dim * 2 ** 3))  ## From Level 4 to Level 3
            self.reduce_chan_level3 = nn.Conv2d(int(dim * 2 ** 3), int(dim * 2 ** 2), kernel_size=1, bias=bias)
            self.decoder_level3 = nn.Sequential(
                *[TransformerBlock(dim=int(dim * 2 ** 2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type, img_size=img_size[2]) for i in range(num_blocks[2])])

            self.up3_2 = Upsample(int(dim * 2 ** 2))  ## From Level 3 to Level 2
            self.reduce_chan_level2 = nn.Conv2d(int(dim * 2 ** 2), int(dim * 2 ** 1), kernel_size=1, bias=bias)
            self.decoder_level2 = nn.Sequential(
                *[TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type, img_size=img_size[1]) for i in range(num_blocks[1])])

            self.up2_1 = Upsample(int(dim * 2 ** 1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)

            self.decoder_level1 = nn.Sequential(
                *[TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type, img_size=img_size[0]) for i in range(num_blocks[0])])

            self.refinement = nn.Sequential(
                *[TransformerBlock(dim=int(dim * 2 ** 1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type, img_size=img_size[0]) for i in range(num_refinement_blocks)])

            #### For Dual-Pixel Defocus Deblurring Task ####
            self.dual_pixel_task = dual_pixel_task
            if self.dual_pixel_task:
                self.skip_conv = nn.Conv2d(dim, int(dim * 2 ** 1), kernel_size=1, bias=bias)
            ###########################

            self.output = nn.Conv2d(int(dim * 2 ** 1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)

        def forward(self, inp_img, distill=False):
            inp_enc_level1 = self.patch_embed(inp_img)
            out_enc_level1 = self.encoder_level1(inp_enc_level1)

            inp_enc_level2 = self.down1_2(out_enc_level1)
            out_enc_level2 = self.encoder_level2(inp_enc_level2)

            inp_enc_level3 = self.down2_3(out_enc_level2)
            out_enc_level3 = self.encoder_level3(inp_enc_level3)

            inp_enc_level4 = self.down3_4(out_enc_level3)
            latent = self.latent(inp_enc_level4)

            inp_dec_level3 = self.up4_3(latent)
            inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)
            inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)
            out_dec_level3 = self.decoder_level3(inp_dec_level3)

            inp_dec_level2 = self.up3_2(out_dec_level3)
            inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)
            inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)
            out_dec_level2 = self.decoder_level2(inp_dec_level2)

            inp_dec_level1 = self.up2_1(out_dec_level2)
            inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)
            out_dec_level1 = self.decoder_level1(inp_dec_level1)

            out_dec_level1 = self.refinement(out_dec_level1)

            if self.global_sc:
                final_output = self.output(out_dec_level1) + inp_img[:, :3, :, :]
            else:
                final_output = self.output(out_dec_level1)

            if distill:
                res = [
                    out_enc_level1, out_enc_level2, out_enc_level3, latent,
                    out_dec_level3, out_dec_level2, out_dec_level1, final_output]
                return res
            else:
                return final_output

    return RestormerEx


##############################################################
##############              D 4 C                 ############
##############################################################
class D4C(nn.Module):
    def __init__(self):
        super(D4C, self).__init__()

        self.restormer = get_restormer()()
        self.hat = get_hat()()
        self.naf = get_xrestormer()()
        self.swinir = get_restormer_ex()()

        for p in self.parameters():
            p.requires_grad = False

        self.rrdb = get_rrdb()()

    def forward(self, x):
        with torch.no_grad():
            out_naf = self.naf(x)
            out_restormer = torch.flip(self.restormer(torch.flip(x, [2])), [2])
            out_swinir = torch.flip(self.swinir(torch.flip(x, [3])), [3])
            out_hat = torch.flip(self.hat(torch.flip(x, [2, 3])), [2, 3])
            out_avg = (out_naf + out_restormer + out_swinir + out_hat) / 4
            out = torch.cat((out_avg, out_naf, out_restormer, out_swinir, out_hat, x), dim=1)
        out = self.rrdb(out)
        return out
